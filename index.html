<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="天天的个人博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="天天的个人博客">
<meta property="article:author" content="tiantian">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>天天的个人博客</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">天天的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/" itemprop="url">吴恩达机器学习（十二）支持向量机SVM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T17:04:00+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="十二、支持向量机-Support-Vector-Machines"><a href="#十二、支持向量机-Support-Vector-Machines" class="headerlink" title="十二、支持向量机 (Support Vector Machines)"></a>十二、支持向量机 (Support Vector Machines)</h2><h3 id="12-1-优化目标"><a href="#12-1-优化目标" class="headerlink" title="12.1 优化目标"></a>12.1 优化目标</h3><p>与<strong>逻辑回归</strong>和<strong>神经网络</strong>相比，<strong>支持向量机</strong>，或者简称<strong>SVM</strong>，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>
<p>也属于监督学习算法</p>
<p>为了描述支持向量机，事实上，我们从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。</p>
<p>如前所述，逻辑回归假设如下：  $h( z )=\frac{1}{1+e^{-\theta^Tx}}​$</p>
<p>图像<img src="http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20%5B1%5D.png" alt="img"></p>
<p>为了解释数学，我们使用上面定义的 $z = \theta^{T}x ​$</p>
<p>我们希望逻辑回归做什么？</p>
<ul>
<li>我们有一个y = 1的例子</li>
<li>我们希望当 $h_\theta(x)​$ 趋近 1<ul>
<li>因为我们想要正确地将此样本分类，这就意味着当 $h_\theta(x)​$趋近于1时，$z = \theta^{T}x ​$ 应当远大于0</li>
</ul>
</li>
<li>当$h_\theta(x)​$ 趋近 0<ul>
<li>对应于$\theta^Tx​$，或者就是 $z​$ 会远小于0</li>
</ul>
</li>
</ul>
<p>让我们换一种思维</p>
<p>$h_\theta \left( x \right)=g\left(\theta^{T}X \right)​$</p>
<p> $g\left( z \right)=\frac{1}{1+e^{-z}}$</p>
<p><img src="/images/1584696526870.png" alt="1584696526870"></p>
<p>类似于上面的例子，每一组数据都会为cost function增加一项，对与总的cost function，我们会将所有的训练集提供的项求和，并乘以$\frac{1}{m}​$</p>
<p>现在先忽略$\frac{1}{m}​$这一项，一起来考虑两种情况：</p>
<ul>
<li><p>$y=1$</p>
<ul>
<li><p>cost function中只有第一项（$y^{(i)}\log ( {h_\theta}(x^{(i)})​$）起作用</p>
</li>
<li><p>因此，当在 $y=1$ 时，得到下面这张图</p>
<p><img src="http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20%5B5%5D.png" alt="img"></p>
</li>
<li><p>该图显示了在给定 $z$ 的情况下 y = 1时示例的成本贡献</p>
<ul>
<li>因此，如果z大，则cost低——这很好！</li>
<li>如果z为0或负数，则cost贡献较高</li>
<li>This is why, when logistic regression sees a positive example, it tries to set $\theta^T x​$ to be a very large term</li>
</ul>
</li>
</ul>
</li>
<li><p>$y=0$</p>
<ul>
<li>cost function中只有第二项$\left( 1-y^{(i)} \right)\log \left( 1-h_\theta\left(x^{(i)} \right) \right)​$起作用</li>
<li>同上得到下图：</li>
</ul>
<p><img src="http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20%5B6%5D.png" alt="img"></p>
<ul>
<li>如果z小则cost低</li>
<li>但是如果z大，那么cost就很大</li>
</ul>
</li>
</ul>
<p><strong>Logistic回归成本函数的SVM成本函数</strong></p>
<p>要构建SVM，我们必须重新定义成本函数，我们以上面的两张图为基础：</p>
<ul>
<li><p>$y=1​$</p>
<ul>
<li>取$y=1​$的函数并创建一个新的cost function</li>
<li>代替旧的cost function曲线，创建两条直线（洋红色），它们近似于逻辑回归y = 1函数</li>
</ul>
<p><img src="http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20%5B7%5D.png" alt="img"></p>
<ul>
<li>在z轴上取点（1）<ul>
<li>从这个点斜率 = 0</li>
</ul>
</li>
<li>这意味着新的曲线是由两条直线组合而成</li>
<li>所以这是新的y = 1 cost function<ul>
<li>使SVM具有计算优势和更轻松的优化问题</li>
<li>我们称此cost function为：$cost_1(z)$</li>
</ul>
</li>
</ul>
</li>
<li><p>$y=0$</p>
<ul>
<li>用y = 0函数图做等效项</li>
</ul>
<p><img src="http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20%5B8%5D.png" alt="img"></p>
<ul>
<li>我们称此cost function为：$cost_2(z)$</li>
</ul>
</li>
</ul>
<p><strong>完整的 SVM cost function</strong></p>
<p><img src="/images/1584696618394.png" alt="1584696618394"></p>
<p><strong>表示方法不同处</strong></p>
<ul>
<li><p>除去$1/m​$这一项</p>
<ul>
<li>去除这一项并不会导致得出的参数出现变化</li>
</ul>
</li>
<li><p>对于逻辑回归的两个条件</p>
<ul>
<li>训练数据集项（$[y^{(i)}cost_1 \left(\theta^Tx^{(i)}) \right)+\left( 1-y^{(i)} \right)cost_0\left(\theta^Tx^{(i)}) \right]​$）=  <strong>A</strong></li>
<li>正则化项（$\frac{1}{2}\sum\limits_{i=1}^{n}\theta^2_j​$）= B<ul>
<li>因此我们可以将其描述为A +  λB</li>
<li>需要某种方式来处理正则化项和训练样本的代价之间的平衡</li>
<li>为λ设置不同的值  以参数化找到这个最好的拟合</li>
</ul>
</li>
<li>参数化为  A +  λB 替换<ul>
<li><strong>对于SVM，约定是使用另一个称为C的参数</strong></li>
<li>CA + B也是如此</li>
<li>如果C等于1 / λ，那么两个函数（CA + B和A +  λB）  将给出相同的值</li>
</ul>
</li>
</ul>
</li>
<li><p>我们最后得到的等式是：$minC\sum\limits_{i=1}^m{[y^{(i)}cost_1 \left(\theta^Tx^{(i)}) \right)+\left( 1-y^{(i)} \right)cost_0\left(\theta^Tx^{(i)}) \right]}+\frac{1}{2}\sum\limits_{i=1}^n\theta^2_j​$</p>
</li>
<li><p>对于假设函数 $h_\theta(x) = 1 \ \ if\ \theta^Tx\ge0​$</p>
<p>​                $h_\theta(x) = 0 \ \ others$</p>
</li>
</ul>
<h3 id="12-2-大边界的直观理解（Large-margin-intuition）"><a href="#12-2-大边界的直观理解（Large-margin-intuition）" class="headerlink" title="12.2 大边界的直观理解（Large margin intuition）"></a>12.2 大边界的直观理解（Large margin intuition）</h3><p>人们有时将<strong>支持向量机</strong>称为<strong>大间距分类器</strong></p>
<p><img src="http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20%5B12%5D.png" alt="img"></p>
<p>上图中我们画出了${\cos}t_1{(z)}$和${\cos}t_0{(z)}​$</p>
<p>现在让我们考虑一下，最小化这些代价函数的必要条件是什么</p>
<ul>
<li><p>If y =1</p>
<ul>
<li>${\cos}t_1{(z)}$ = 0 only when z &gt;= 1</li>
</ul>
</li>
<li><p>If y = 0</p>
<ul>
<li>${\cos}t_0{(z)}​$ = 0 only when z &lt;= -1</li>
</ul>
</li>
<li><p>这里面有一个有趣的地方</p>
<ul>
<li>如果你有一个正样本$y=1​$，则其实我们仅仅要求 $z​$ 大于等于0，就能将该样本恰当的分类</li>
<li>类似地，如果你有一个负样本，则仅需要总$z$ &lt;=0就会将负例正确分类</li>
<li>但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求 $z$ &gt;0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1</li>
<li>也就是说需要在支持向量机中嵌入了一个<strong>额外的安全因子</strong></li>
</ul>
</li>
</ul>
<p><strong>我们想知道，在SVM中，这个因子会导致什么结果</strong></p>
<p><img src="/images/1584696727001.png" alt="1584696727001"></p>
<p>我们接下来会考虑一个特例：将这个常数C设置成一个非常大的值</p>
<p>如果 $C$非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项(中括号内)为0的最优解。</p>
<p>我们已经看到输入一个训练样本标签为$y=1$，你想令${\cos}t_1{(z)}$为0，你需要做的是找到一个$$，使得$\theta^Tx&gt;=1$；类似地，对于一个训练样本，标签为$y=0$，为了使${\cos}t_0{(z)}$ 函数的值为0，我们需要$\theta^Tx&lt;=-1$</p>
<p><strong>我们要在同时满足上面的$y=1$和$y=0$的情况下找到能使第一项A为0的参数$\theta$</strong></p>
<p>现在考虑我们的优化问题</p>
<p>由于第一项(A)等于0，所以我们可以将这一项删去，我们所需要最小化的部分也发生了变化</p>
<p>$min\frac{1}{2}\sum\limits_{i=1}^{n}\theta^2_j​$<br>遵从：$\theta^Tx^{(i)}&gt;=1​$，如果 $y^{(i)}=1​$</p>
<pre><code>$\theta^Tx^{(i)}&lt;=-1$，如果 $y^{(i)}=0$</code></pre><p>现在我们来求解这个优化问题的时候，会得到一个十分有趣的决策边界<br><img src="https://img-blog.csdnimg.cn/20200216202054167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDgyMTQ4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>绿线和品红色线是可以通过逻辑回归选择的功能决策边界，<ul>
<li>但是他们可能没有很好地分类数据</li>
</ul>
</li>
<li>黑线是SVM选择的<ul>
<li>SVM是更强大的分类器</li>
</ul>
</li>
<li>数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做间距(<strong>margin</strong>)。</li>
</ul>
<p>事实上，支持向量机现在要比这个大间距分类器所体现得更成熟更复杂，尤其是当你使用大间距分类器的时候，你的学习算法会受<strong>异常点(outlier)</strong> 的影响。</p>
<p>比如我们加入一个额外的正样本<br><img src="https://img-blog.csdnimg.cn/20200216202109997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDgyMTQ4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>加入异常点前我们拟合得到的使黑色的线，而加入异常点之后我们得到的使品红色的线，这样的结果显然很糟糕</p>
<p><strong>但如果你将C设置的不要太大，则你最终会得到这条黑线</strong></p>
<p>也就是说<strong>当$C$不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界，而只有当你的数据中没有异常点并且可以轻松的线性分离数据时，才适合用SVM作为大间距分类器</strong></p>
<p>回顾 $C=1/\lambda$，因此：</p>
<p>$C$ 较大时，相当于 $\lambda$ 较小，可能会导致过拟合，高方差。</p>
<p>$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差。</p>
<p>稍后会介绍支持向量机的偏差和方差，希望在那时候关于如何处理参数的这种平衡会变得更加清晰</p>
<h3 id="12-4-核函数1：使SVM适应非线性分类器"><a href="#12-4-核函数1：使SVM适应非线性分类器" class="headerlink" title="12.4 核函数1：使SVM适应非线性分类器"></a>12.4 核函数1：使SVM适应非线性分类器</h3><p>回顾我们之前讨论过可以使用高次项的多项式模型来解决无法用直线进行分隔的分类问题：<br><img src="https://img-blog.csdnimg.cn/20200216202139908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDgyMTQ4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为了获得上图所示的判定边界，我们的模型可能是${{\theta }_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+{{\theta }_{3}}{{x}_{1}}{{x}_{2}}+{{\theta }_{4}}x_{1}^{2}+{{\theta }_{5}}x_{2}^{2}+\cdots​$的形式。</p>
<p>我们可以用一系列的新的特征$f​$来替换模型中的每一项。例如令：<br>${{f}_{1}}={{x}_{1}},{{f}_{2}}={{x}_{2}},{{f}_{3}}={{x}_{1}}{{x}_{2}},{{f}_{4}}=x_{1}^{2},{{f}_{5}}=x_{2}^{2}​$…得到$h_θ(x)={{\theta }_{1}}f_1+{{\theta }_{2}}f_2+…+{{\theta }_{n}}f_n​$。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造$f_1,f_2,f_3​$ ？（这样的构造方式在x过多时计算量太大）</p>
<p>我们可以利用<strong>核函数</strong>来计算出新的特征。</p>
<p>给定一个训练样本$x​$，我们利用$x​$的各个特征与我们预先选定的<strong>标记</strong>(<strong>landmarks</strong>)$l^{(1)},l^{(2)},l^{(3)}​$的近似程度来选取新的特征$f_1,f_2,f_3​$</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5ob2xlaG91c2Uub3JnL21sY2xhc3MvMTJfU3VwcG9ydF9WZWN0b3JfTWFjaGluZXNfZmlsZXMvSW1hZ2UlMjBbNDFdLnBuZw?x-oss-process=image/format,png" alt="img"></p>
<p>例如：${{f}_{1}}=similarity(x,{{l}^{(1)}})=e(-\frac{{{\left\| x-{{l}^{(1)}} \right|}^{2}}}{2{{\sigma }^{2}}})$ </p>
<pre><code>${{f}_{2}}=similarity(x,{{l}^{(2)}})=e(-\frac{{{\left\| x-{{l}^{(2)}} \right\|}^{2}}}{2{{\sigma }^{2}}})$ </code></pre><p>​       ……</p>
<p><img src="/images/1584696858952.png" alt="1584696858952"></p>
<p>上例中的$similarity(x,l^{(1)})$就是<strong>核函数</strong>（一种相似度的度量标准）</p>
<p>具体而言，这是一个<strong>高斯核函数</strong>(<strong>Gaussian Kernel</strong>)。 <strong>注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。</strong></p>
<p><strong>这些标记的作用是什么？</strong></p>
<p>如果一个训练样本$x​$与地标$l​$之间的距离近似于0，则新特征 $f​$ 近似于$e^{-0}=1​$，如果训练样本$x​$与地标$l​$之间距离较远，则$f​$近似于$e^{-(一个较大的数)}=0​$。</p>
<p>假设我们的训练样本含有两个特征[$x_{1}$ $x{_2}$]，给定标记$l^{(1)}$与不同的 $\sigma$ 值，见下图：</p>
<ul>
<li>$\sigma$ 是 <strong>标准差</strong> </li>
<li>$\sigma^2​$ 通常被称为 <strong>方差</strong>，这个数值越小，标记(landmarks)周围上升的陡度越大<br><img src="https://img-blog.csdnimg.cn/20200216202218233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDgyMTQ4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>图中水平面的坐标为 $x_{1}​$，$x_{2}​$，而垂直坐标轴代表$f​$。可以看出，只有当$x​$与$l^{(1)}​$重合时$f​$才具有最大值。随着$x​$的改变$f​$值改变的速率受到$\sigma^2​$的控制。</li>
</ul>
<p>在下图中，假设我们已经运行了算法并获得了 $\theta$ 值</p>
<p>当样本处于<strong>品红色</strong>的点位置处，因为其离$l^{(1)}​$更近，但是离$l^{(2)}​$和$l^{(3)}​$较远，因此$f_1​$接近1，而$f_2​$,$f_3​$接近0。因此$h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3&gt;0​$，因此预测$y=1​$。</p>
<p>对于<strong>蓝绿色</strong>的点，因为其离三个标记都较远，预测$y=0$。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5ob2xlaG91c2Uub3JnL21sY2xhc3MvMTJfU3VwcG9ydF9WZWN0b3JfTWFjaGluZXNfZmlsZXMvSW1hZ2UlMjBbNDldLnBuZw?x-oss-process=image/format,png" alt="img"></p>
<p>这样，下图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练样本和我们选取的标记所得出的判定边界，在预测时，我们采用的特征不是训练样本本身的特征，而是通过核函数计算出的新特征$f_1,f_2,f_3$。</p>
<p><img src="F:%5Cworkspace%5Cpython%5C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%5C%E5%90%B4%E6%81%A9%E8%BE%BE%5Ctiantian152-Coursera-ML-AndrewNg-Notes-master%5CCoursera-ML-AndrewNg-Notes%5Cimages%5C3d8959d0d12fe9914dc827d5a074b564.jpg" alt="3d8959d0d12fe9914dc827d5a074b564"></p>
<ul>
<li><p>在内部我们预测y = 1</p>
</li>
<li><p>外面我们预测y = 0</p>
</li>
<li><p>因此，这说明了我们如何在支持向量机中创建具有界标和核函数的非线性边界</p>
</li>
</ul>
<p>但是现在这个模型仍然存在没有解决的问题</p>
<ul>
<li>我们如何获得/选择 地标</li>
<li>我们还可以使用其他哪些内核（高斯内核除外）<h3 id="12-5-核函数2"><a href="#12-5-核函数2" class="headerlink" title="12.5 核函数2"></a>12.5 核函数2</h3></li>
</ul>
<h5 id="如何选择标记-landmarks"><a href="#如何选择标记-landmarks" class="headerlink" title="如何选择标记(landmarks)"></a>如何选择标记(landmarks)</h5><p>我们通常是根据训练集的数量选择标记的数量，即如果训练集中有$m$个样本，则我们选取$m$个标记，并且令：$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…..,l^{(m)}=x^{(m)}$。</p>
<p>这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：<img src="https://img-blog.csdnimg.cn/20200216201536961.png" alt="eca2571849cc36748c26c68708a7a5bd"></p>
<p>下面我们将核函数运用到<strong>支持向量机（SVM）</strong>中，修改我们的支持向量机假设为：</p>
<p>给定$x​$，计算新特征$f​$，当$θ^Tf&gt;=0​$ 时，预测 $y=1​$，否则反之。 </p>
<p>相应地修改代价函数为：$\sum\limits_{j=1}^{n=m}\theta _j^2=\theta^{T}\theta​$，</p>
<p><img src="/images/1584696927439.png" alt="1584696927439"><br>在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算$\sum\limits_{j=1}^{n=m}\theta _j^2=\theta^{T}\theta$时，我们用$θ^TMθ$代替$θ^Tθ$，其中$M$是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了<strong>简化计算</strong>。</p>
<p>理论上讲，我们也可以在<strong>逻辑回归</strong>中使用核函数，但是上面使用 $M$来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</p>
<p>在此，我们不介绍<strong>最小化支持向量机的代价函数</strong>的方法，你可以使用现有的软件包（如<strong>liblinear</strong>,<strong>libsvm</strong>等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行<strong>特征缩放</strong>是非常必要的。</p>
<p>另外，支持向量机也可以不使用核函数，不使用核函数又称为<strong>线性核函数</strong>(<strong>linear kernel</strong>)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。</p>
<p>下面是支持向量机的两个参数$C$和$\sigma$的影响：</p>
<p>$C=1/\lambda$</p>
<p>$C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差；</p>
<p>$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差；</p>
<p>$\sigma$ 较大时，可能会导致低方差，高偏差，f 特征更加平稳</p>
<p>$\sigma$ 较小时，可能会导致低偏差，高方差，f 特征变化较大</p>
<p>如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。</p>
<h3 id="12-6-使用支持向量机"><a href="#12-6-使用支持向量机" class="headerlink" title="12.6 使用支持向量机"></a>12.6 使用支持向量机</h3><p>需要的东西</p>
<ul>
<li>使用SVM软件包（例如liblinear，libsvm）求解参数 $\theta$</li>
<li>需要指定一些参数<ul>
<li>参数C</li>
<li>内核<ul>
<li>高斯核函数(<strong>Gaussian Kernel</strong>)</li>
<li>线性核函数</li>
<li>多项式核函数（<strong>Polynomial Kerne</strong>l）</li>
<li>字符串核函数（<strong>String kernel</strong>）</li>
<li>卡方核函数（ <strong>chi-square kernel</strong>）</li>
<li>直方图交集核函数（<strong>histogram intersection kernel</strong>）</li>
<li>等等…</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>下面是一些普遍使用的准则：</strong></p>
<p>$n$为特征数，$m$为训练样本数。</p>
<p>(1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</p>
<p>(2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。</p>
<p>(3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</p>
<p>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1/" itemprop="url">吴恩达机器学习（十一）机器学习系统的设计</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T17:02:00+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="十一、机器学习系统的设计"><a href="#十一、机器学习系统的设计" class="headerlink" title="十一、机器学习系统的设计"></a>十一、机器学习系统的设计</h2><h3 id="11-0-工作的优先级：垃圾邮件分类例子"><a href="#11-0-工作的优先级：垃圾邮件分类例子" class="headerlink" title="11.0 工作的优先级：垃圾邮件分类例子"></a>11.0 工作的优先级：垃圾邮件分类例子</h3><p>我们将垃圾邮件标注为 spam 用1表示</p>
<p>将废垃圾邮件标注为 non-spam 用0表示</p>
<p>如果我们有一些这样标注好的垃圾和非垃圾邮件样本，如何来训练一个垃圾邮件分类器？很清楚这是一个有监督学习的问题，假设我们选择逻辑回归算法来训练这样的分类器，首先必须选择合适的特征。这里定义：</p>
<ul>
<li>x = 邮件的特征；</li>
<li>y = 垃圾邮件(1) 或 非垃圾邮件(0)</li>
</ul>
<p>我们可以选择100个典型的词汇集合来代表垃圾/非垃圾（单词），例如deal, buy, discount, andrew, now等，可以按它们的字母顺序排序。对于已经标注好的邮件训练样本，如果100个词汇中有单词j在样本中出现，就用1代表特征向量x中的xj，否则用0表示，这样训练样本就被特征向量x所替代:</p>
<p>注意在实际使用中，我们不会手动去选择100个典型的词汇，而是从训练集中选择出现频率最高的前n个词，例如10000到50000个。</p>
<p>那么，如何高效的训练一个垃圾邮件分类器使其准确率较高，错误率较小？</p>
<ul>
<li><p>首先很自然的考虑到收集较多的数据，例如”honeypot” project,一个专门收集垃圾邮件服务器ip和垃圾邮件内容的项目；</p>
</li>
<li><p>但是上一章已经告诉我们，数据并不是越多越好，所以可以考虑设计其他复杂的特征，例如利用邮件的发送信息，这通常隐藏在垃圾邮件的顶部；</p>
</li>
<li><p>还可以考虑设计基于邮件主体的特征，例如是否将”discount”和”discounts”看作是同一个词？同理如何处理”deal”和”Dealer”? 还有是否将标点作为特征？</p>
</li>
<li><p>最后可以考虑使用复杂的算法来侦测错误的拼写（垃圾邮件会故意将单词拼写错误以逃避垃圾邮件过滤器，例如m0rtgage, med1cine, w4tches)</p>
</li>
</ul>
<h3 id="11-1-首先要做什么"><a href="#11-1-首先要做什么" class="headerlink" title="11.1 首先要做什么"></a>11.1 首先要做什么</h3><p>以一个垃圾邮件分类器算法为例进行讨论。</p>
<p>为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量$x​$。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。</p>
<p>为了构建这个分类器算法，我们可以做很多事，例如：</p>
<ol>
<li><strong>收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</strong></li>
<li><strong>基于邮件的路由信息开发一系列复杂的特征</strong></li>
<li><strong>基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</strong></li>
<li><strong>为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法</strong></li>
</ol>
<p>在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。</p>
<p>我们将在随后的课程中讲误差分析，我会告诉你怎样用一个更加系统性的方法，从一堆不同的方法中，选取合适的那一个。因此，你更有可能选择一个真正的好方法，能让你花上几天几周，甚至是几个月去进行深入的研究。</p>
<h3 id="11-2-误差分析（Error-Analysis）"><a href="#11-2-误差分析（Error-Analysis）" class="headerlink" title="11.2 误差分析（Error Analysis）"></a>11.2 误差分析（Error Analysis）</h3><p>构建一个学习算法的推荐方法为：</p>
<ol>
<li><p>从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</p>
</li>
<li><p>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</p>
</li>
<li><p>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势</p>
</li>
</ol>
<p>假设交叉验证集上有500个邮件样本，其中算法错分了100个邮件，那么我们就人工来检查这100个bad case, 并且按如下的方式对它们进行分类：</p>
<ul>
<li>(i) 邮件是什么类型的？</li>
<li>(ii) 什么样的线索或特征你认为有可能对算法的正确分类有帮助？</li>
</ul>
<p>数值评估的重要性：</p>
<p>在对bad case进行分析后，我们可能会考虑如下的方法：</p>
<ul>
<li>对于discount/discounts/discounted/discounting 能否将它们看作是同一个词？</li>
<li>能不能使用“词干化”的工具包来取单词的词干，例如“Porter stemmer”？</li>
</ul>
<p>错误分析不能决定上述方法是否有效，它只是提供了一种解决问题的思路和参考，只有在实际的尝试后才能看出这些方法是否有效。</p>
<p>所以我们需要对算法进行数值评估（例如交叉验证集误差），来看看使用或不使用某种方法时的算法效果，例如：</p>
<ul>
<li>不对单词提前词干：5%错误率   vs 对单词提取词干：3% 错误率</li>
<li>对大小写进行区分（Mom / mom): 3.2% 错误率</li>
</ul>
<h3 id="11-3-不对称性分类的错误评估-Error-metrics-for-skewed-classes"><a href="#11-3-不对称性分类的错误评估-Error-metrics-for-skewed-classes" class="headerlink" title="11.3 不对称性分类的错误评估(Error metrics for skewed classes)"></a>11.3 不对称性分类的错误评估(Error metrics for skewed classes)</h3><p>什么是<strong>不对称性分类</strong>？</p>
<p>以癌症预测或者分类为例，我们训练了一个逻辑回归模型. 如果是癌症，y = 1, 其他则 y = 0。<br>在测试集上发现这个模型的错误率仅为1%（99%都分正确了），貌似是一个非常好的结果？<br>但事实上，仅有0.5%的病人得了癌症，如果我们不用任何学习算法，对于测试集中的所有人都预测y = 0，既没有癌症。那么这个预测方法的错误率仅为0.5%，比我们废好大力训练的逻辑回归模型的还要好。这就是一个不对称分类的例子，对于这样的例子，<strong>仅仅考虑错误率是有风险的</strong>。</p>
<p>现在我们就来考虑一种标准的衡量方法：<strong>Precision/Recall(精确度和召回率)</strong></p>
<p>首先对正例和负例做如下的定义：<img src="http://52opencourse.com/?qa=blob&qa_blobid=5923680306589653738" alt="正负例问题-我爱公开课-52opencourse.com"></p>
<p>其中：</p>
<p>True Positive （真正例, TP）被模型预测为正的正样本；可以称作判断为真的正确率</p>
<p>True Negative（真负例 , TN）被模型预测为负的负样本 ；可以称作判断为假的正确率</p>
<p>False Positive （假正例, FP）被模型预测为正的负样本；可以称作误报率</p>
<p>False Negative（假负例 , FN）被模型预测为负的正样本；可以称作漏报率  </p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th><strong>预测值</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td><strong>Positive</strong></td>
<td><strong>Negtive</strong></td>
</tr>
<tr>
<td><strong>实际值</strong></td>
<td><strong>Positive</strong></td>
<td><strong>TP</strong></td>
<td><strong>FN</strong></td>
</tr>
<tr>
<td></td>
<td><strong>Negtive</strong></td>
<td><strong>FP</strong></td>
<td><strong>TN</strong></td>
</tr>
</tbody></table>
<p>那么对于癌症预测这个例子我们可以定义：</p>
<p>Precision（精确度）-预测中实际得癌症的病人数量(真正例)除以我们预测的得癌症的病人数量：</p>
<p>$\frac{True\ positives}{predocted\ positive}=\frac{True\ Positive }{True\ Positive + False\ Positive}$</p>
<p><strong>Precision = TP/(TP+FP)</strong></p>
<p>Recall（召回率）-预测中实际得癌症的病人数量(真正例)除以实际得癌症的病人数量：</p>
<p>$\frac{True\ positives}{actual\ positive}=\frac{True\ Positive }{True\ Positive + False\ Negative}​$</p>
<p><strong>Recall = TP/(TP+FN)</strong></p>
<h3 id="11-4-精确度和召回率的权衡-Trading-off-precision-and-recall"><a href="#11-4-精确度和召回率的权衡-Trading-off-precision-and-recall" class="headerlink" title="11.4 精确度和召回率的权衡(Trading off precision and recall)"></a>11.4 精确度和召回率的权衡(Trading off precision and recall)</h3><p>假设我们的分类器使用了逻辑回归模型，预测值在0到1之间:， 一种通常的判断正负例的方法是设置一个阈值，例如0.5：</p>
<ul>
<li>如果 ，则预测为1， 正例；</li>
<li>如果 , 则预测为0， 负例；</li>
</ul>
<p>这个时候，我们就可以计算这个分类器的<strong>precision and recall(精确度和召回率)</strong>:</p>
<ul>
<li><p><strong>Precision = TP/(TP+FP)</strong> 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p>
</li>
<li><p><strong>Recall = TP/(TP+FN)</strong> 例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
</li>
</ul>
<p>这个时候，不同的阈值会导致不同的精确度和召回率，那么如何来权衡这二值？</p>
<p>对于癌症预测这个例子:</p>
<p>假设我们非常有把握时才预测病人得癌症（y=1)， 这个时候，我们常常将阈值设置的很高，这会导致高精确度，低召回率(Higher precision, lower recall);</p>
<p>假设我们不希望将太多的癌症例子错分（避免假负例，本身得了癌症，确被分类为没有得癌症）, 这个时候，阈值就可以设置的低一些，这又会导致高召回率，低精确度(Higher recall, lower precision);</p>
<p>这些问题，可以归结到一张Precision Recall曲线，简称PR-Curve:</p>
<p><img src="http://52opencourse.com/?qa=blob&qa_blobid=333243597623131971" alt="Precision Recall 曲线-PR 曲线-我爱公开课-52opencourse.com"></p>
<p>通常我们会考虑用它们的均值来做比较，但是这会引入一个问题，例如上面三组Precision/Recall的均值分别是：0.45, 0.4, 0.51，最后一组最好，但是最后一组真的好吗？如果我们将阈值定的很低，甚至为0， 那么对于所有的测试集，我们的预测都是y = 1, 那么recall 就是1.0，我们根本就不需要什么复杂的机器学习算法，直接预测y = 1就得了，所以，用Precison/Recall的均值不是一个好办法。</p>
<p>我们希望有一个帮助我们选择这个阀值的方法。一种方法是计算<strong>标准的F值或者F1-score:</strong></p>
<p>其计算公式为：${F}_{1}Score:2\frac{PR}{P+R}$</p>
<p><strong>我们选择使得F1值最高的阀值。</strong></p>
<p>F值是对精确度和召回率的一个很好的权衡，两种极端的情况也能很好的平衡</p>
<h3 id="11-5-数据对于机器学习的重要性（Data-for-machine-learning）"><a href="#11-5-数据对于机器学习的重要性（Data-for-machine-learning）" class="headerlink" title="11.5 数据对于机器学习的重要性（Data for machine learning）"></a>11.5 数据对于机器学习的重要性（Data for machine learning）</h3><p>在设计一个高准确率的机器学习系统时，数据具有多大的意义？ 2001年的时候，Banko and Brill曾做了一个实验，对易混淆的单词进行分类，也就是在一个句子的上下文环境中选择一个合适的单词，例如：<br>For breakfast I ate ___ eggs  </p>
<p>给定{to, two, too}，选择一个合适的单词。<br>他们用了如下几种机器学习算法：</p>
<ul>
<li>-Perceptron(Logistic regression)</li>
<li>-Winnow</li>
<li>-Memory-based</li>
<li>-Naïve Bayes</li>
</ul>
<p>根据训练集的不同规模记录这几种算法的准确率，并且做了如下的图：</p>
<p><img src="http://52opencourse.com/?qa=blob&qa_blobid=9727548652199519336" alt="数据对于机器学习的意义"></p>
<p>最终得到的结论是：</p>
<p><strong>“It’s not who has the best algorithm that wins. It’s who has the most data.”</strong></p>
<p>选择大数据的理由？</p>
<p>假设我们的特征有很多的信息来准确的预测y, 例如，上面的易混淆词分类的例子，它有整个句子的上下文可以利用；</p>
<p>反过来，例如预测房价的时候，如果仅有房屋大小这个特征，没有其他的特征，能预测准确吗？</p>
<p>对于这样的问题，一种简单的测试方法是给定这样的特征，一个人类专家能否准确的预测出y?</p>
<p>如果一个学习算法有很多的参数，例如逻辑回归/线性回归有很多的特征，神经网络有很多隐藏的单元，那么它的训练集误差将会很小，但容易陷入过拟合；如果再使用很大的训练数据，那么它将很难过拟合，它的训练集误差和测试集误差将会近似相等，并且很小。所以大数据对于机器学习还是非常重要的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%EF%BC%89%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%EF%BC%89%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/" itemprop="url">吴恩达机器学习（十）应用机器学习的建议</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T17:00:00+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="十、应用机器学习的建议"><a href="#十、应用机器学习的建议" class="headerlink" title="十、应用机器学习的建议"></a>十、应用机器学习的建议</h2><h3 id="10-1-决定下一步做什么"><a href="#10-1-决定下一步做什么" class="headerlink" title="10.1 决定下一步做什么"></a>10.1 决定下一步做什么</h3><p>由于可能没有完全理解怎样运用这些算法。因此总是把时间浪费在毫无意义的尝试上。我想做的是确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。具体来讲，我将重点关注的问题是假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？为了解释这一问题，我想仍然使用预测房价的学习例子，假如你已经完成了正则化线性回归，也就是最小化代价函数$J​$的值，假如，在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在你的问题是要想改进这个算法，接下来应该怎么办？</p>
<p>实际上你可以想出很多种方法来改进这个算法的性能，其中一种办法是使用更多的训练样本。具体来讲，也许你能想到通过电话调查或上门调查来获取更多的不同的房屋出售数据。遗憾的是，我看到好多人花费了好多时间想收集更多的训练样本。他们总认为，要是我有两倍甚至十倍数量的训练数据，那就一定会解决问题的是吧？但有时候获得更多的训练数据实际上并没有作用。</p>
<p>我们也将知道怎样避免把过多的时间浪费在收集更多的训练数据上，这实际上是于事无补的。另一个方法，你也许能想到的是尝试选用更少的特征集。因此如果你有一系列特征比如$x_1,x_2,x_3$等等。也许有很多特征，也许你可以花一点时间从这些特征中仔细挑选一小部分来防止过拟合。或者也许你需要用更多的特征，也许目前的特征集，对你来讲并不是很有帮助。你希望从获取更多特征的角度来收集更多的数据，同样地，你可以把这个问题扩展为一个很大的项目，比如使用电话调查来得到更多的房屋案例，或者再进行土地测量来获得更多有关，这块土地的信息等等，因此这是一个复杂的问题。同样的道理，我们非常希望在花费大量时间完成这些工作之前，我们就能知道其效果如何。我们也可以尝试增加多项式特征的方法，比如$x_1$的平方，$x_2$的平方，$x_1,x_2$的乘积，我们可以花很多时间来考虑这一方法，我们也可以考虑其他方法减小或增大正则化参数$\lambda$的值。我们列出的这个单子，上面的很多方法都可以扩展开来扩展成一个六个月或更长时间的项目。遗憾的是，大多数人用来选择这些方法的标准是凭感觉的，也就是说，大多数人的选择方法是随便从这些方法中选择一种，比如他们会说“噢，我们来多找点数据吧”，然后花上六个月的时间收集了一大堆数据，然后也许另一个人说：“好吧，让我们来从这些房子的数据中多找点特征吧”。我很遗憾不止一次地看到很多人花了至少六个月时间来完成他们随便选择的一种方法，而在六个月或者更长时间后，他们很遗憾地发现自己选择的是一条不归路。幸运的是，有一系列简单的方法能让你事半功倍，排除掉单子上的至少一半的方法，留下那些确实有前途的方法，同时也有一种很简单的方法，只要你使用，就能很轻松地排除掉很多选择，从而为你节省大量不必要花费的时间。最终达到改进机器学习系统性能的目的假设我们需要用一个线性回归模型来预测房价，当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？</p>
<ol>
<li>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</li>
<li>尝试减少特征的数量</li>
<li>尝试获得更多的特征</li>
<li>尝试增加多项式特征</li>
<li>尝试减少正则化程度$\lambda$</li>
<li>尝试增加正则化程度$\lambda$</li>
</ol>
<p>我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。</p>
<p>在接下来的两段视频中，我首先介绍怎样评估机器学习算法的性能，然后在之后的几段视频中，我将开始讨论这些方法，它们也被称为”机器学习诊断法”。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。在这一系列的视频中我们将介绍具体的诊断法，但我要提前说明一点的是，这些诊断法的执行和实现，是需要花些时间的，有时候确实需要花很多时间来理解和实现，但这样做的确是把时间用在了刀刃上，因为这些方法让你在开发学习算法时，节省了几个月的时间，因此，在接下来几节课中，我将先来介绍如何评价你的学习算法。在此之后，我将介绍一些诊断法，希望能让你更清楚。在接下来的尝试中，如何选择更有意义的方法。</p>
<h3 id="10-2-评估一个假设"><a href="#10-2-评估一个假设" class="headerlink" title="10.2 评估一个假设"></a>10.2 评估一个假设</h3><p>评估一个数据集拟合的是否可取，不能只看是否有误差最小化，还要看是否过拟合</p>
<p>判断过拟合有以下两种方法：</p>
<ul>
<li><p>对假设函数$h(x)​$进行画图，然后观察图形趋势</p>
</li>
<li><p>将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。</p>
<p>很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。</p>
<p>测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p>
<ol>
<li><p>对于线性回归模型，我们利用测试集数据计算代价函数$J$</p>
</li>
<li><p>对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外：</p>
<p><img src="/images/1584696217092.png" alt="1584696217092"></p>
</li>
</ol>
<p>误分类的比率，对于每一个测试集样本，计算：</p>
<p><img src="/images/751e868bebf4c0bf139db173d25e8ec4.png" alt=""></p>
<p>然后对计算结果求平均。</p>
</li>
</ul>
<h3 id="10-3-模型选择和交叉验证集"><a href="#10-3-模型选择和交叉验证集" class="headerlink" title="10.3 模型选择和交叉验证集"></a>10.3 模型选择和交叉验证集</h3><p>假设我们要在10个不同次数的二项式模型之间进行选择：</p>
<p><img src="/images/1b908480ad78ee54ba7129945015f87f.jpg" alt=""></p>
<p>显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。</p>
<p>即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集</p>
<p>模型选择的方法为：</p>
<ol>
<li><p>使用训练集训练出10个模型</p>
</li>
<li><p>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</p>
</li>
<li><p>选取代价函数值最小的模型</p>
</li>
<li><p>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</p>
<p><img src="/images/1584696232996.png" alt="1584696232996"></p>
</li>
</ol>
<h3 id="10-4-诊断偏差和方差"><a href="#10-4-诊断偏差和方差" class="headerlink" title="10.4 诊断偏差和方差"></a>10.4 诊断偏差和方差</h3><p>当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？</p>
<p>我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：</p>
<p><img src="/images/bca6906add60245bbc24d71e22f8b836.png" alt=""></p>
<p><img src="/images/1584696246988.png" alt="1584696246988"></p>
<p>对于训练集，当 $d$ （选择的次数）较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。 </p>
<p>对于交叉验证集，当 $d​$ 较小时，模型拟合程度低，误差较大；但是随着 $d​$ 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。</p>
<p>训练集误差和交叉验证集误差近似时：偏差/欠拟合</p>
<p>交叉验证集误差远大于训练集误差时：方差/过拟合</p>
<h3 id="10-5-正则化和偏差-方差"><a href="#10-5-正则化和偏差-方差" class="headerlink" title="10.5 正则化和偏差/方差"></a>10.5 正则化和偏差/方差</h3><p>在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。</p>
<p>我们选择一系列的想要测试的 $\lambda$ 值，通常是 0-10之间的呈现2倍关系的值（如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。</p>
<p><img src="/images/8f557105250853e1602a78c99b2ef95b.png" alt=""></p>
<p>选择$\lambda​$的方法为：</p>
<ol>
<li>使用训练集训练出12个不同程度正则化的模型</li>
<li>用12个模型分别对交叉验证集计算的出交叉验证误差</li>
<li>选择得出交叉验证误差<strong>最小</strong>的模型</li>
<li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：</li>
</ol>
<p><img src="/images/38eed7de718f44f6bb23727c5a88bf5d.png" alt=""></p>
<p>• 当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大</p>
<p>• 随着 $\lambda​$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</p>
<h3 id="10-6-学习曲线"><a href="#10-6-学习曲线" class="headerlink" title="10.6 学习曲线"></a>10.6 学习曲线</h3><p>学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的<strong>合理检验</strong>（<strong>sanity check</strong>）。学习曲线是将训练集误差和交叉验证集误差作为训练集样本数量（$m$）的函数绘制的图表。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/" itemprop="url">吴恩达机器学习（九）神经网络的学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:59:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="九、神经网络的学习"><a href="#九、神经网络的学习" class="headerlink" title="九、神经网络的学习"></a>九、神经网络的学习</h2><h3 id="9-1-代价函数"><a href="#9-1-代价函数" class="headerlink" title="9.1 代价函数"></a>9.1 代价函数</h3><p><img src="%5Cimage%5CIMG_0951(20200207-183936).PNG" alt=""><br>$$h_\theta\left(x\right)\in \mathbb{R}^{K}​$$ $${\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{output}​$$</p>
<p>$J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log {(h_\Theta(x^{(i)}))}<em>k + \left( 1 - y_k^{(i)} \right) \log \left( 1- {\left( h_\Theta \left( x^{(i)} \right) \right)}_k \right) \right] + \frac{\lambda}{2m} \sum\limits</em>{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left( \Theta_{ji}^{(l)} \right)^2​$</p>
<p>如果是矩阵形式，所有的求和号去掉即可</p>
<p>这个看起来复杂很多的代价函数<strong>背后的思想还是一样</strong>的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出K个预测，基本上我们可以利用循环，对每一行特征都预测K个不同结果，然后在利用循环在K个预测中选择可能性最高的一个，将其与y中的实际数据进行比较。</p>
<p>注意正则化那一项，其实就是排除每一层的theta0后，每一层的theta矩阵的和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">''' cost fn is -l(theta) for you to minimize'''</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(-y * np.log(sigmoid(X @ theta)) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - sigmoid(X @ theta)))</span><br><span class="line">   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">'''you don't penalize theta_0'''</span></span><br><span class="line">    theta_j1_to_n = theta[<span class="number">1</span>:]</span><br><span class="line">    regularized_term = (l / (<span class="number">2</span> * len(X))) * np.power(theta_j1_to_n, <span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost(theta, X, y) + regularized_term</span><br></pre></td></tr></table></figure>



<h3 id="9-2-反向传播算法"><a href="#9-2-反向传播算法" class="headerlink" title="9.2 反向传播算法"></a>9.2 反向传播算法</h3><ul>
<li><p>前向传播算法</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/2ea8f5ce4c3df931ee49cf8d987ef25d.png" alt="img"></p>
</li>
<li><p>反向传播算法</p>
<p>$l$ 代表目前所计算的是第几层。</p>
<p>$j$ 代表目前计算层中的激活单元的下标，也将是下一层的第$j$个输入变量的下标。</p>
<p>$i$ 代表下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。</p>
<p><strong>我们以上图为例</strong></p>
<p>我们从<strong>最后一层</strong>的误差开始计算，误差是激活单元的预测（${a^{(4)}}$）与实际值（$y^k$）之间的误差，（$k=1:k$）。</p>
<ol>
<li><p>我们用$\delta​$来表示误差，则：$\delta^{(4)}=a^{(4)}-y​$</p>
</li>
<li><p>我们利用这个误差值来计算前一层的误差：$\delta^{(3)}=\left({\Theta^{(3)}}\right)^{T}\delta^{(4)}\ast g’\left(z^{(3)}\right)​$<br>其中 $g’(z^{(3)})​$是 $S​$ 形函数的导数，$g’(z^{(3)})=a^{(3)}\ast(1-a^{(3)})​$。而$(θ^{(3)})^{T}\delta^{(4)}​$则是权重导致的误差的和。</p>
</li>
<li><p>下一步是继续计算第二层的误差：$ \delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}\ast g’(z^{(2)})$</p>
</li>
<li><p>因为第一层是输入变量，不存在误差。</p>
</li>
<li><p>我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0​$，即我们不做任何正则化处理时有：$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_{j}^{(l)} \delta_{i}^{l+1}​$</p>
</li>
<li><p>如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用$\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 $l$ 层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差。</p>
<ol>
<li><p>首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。</p>
</li>
<li><p>在求出了$\Delta_{ij}^{(l)}​$之后，我们便可以计算代价函数的偏导数了，计算方法如下：</p>
<pre><code>$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}​$              ${if}\; j \neq  0​$</code></pre><p>$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$                             ${if}; j = 0$</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="9-3-反向传播算法的直观理解"><a href="#9-3-反向传播算法的直观理解" class="headerlink" title="9.3 反向传播算法的直观理解"></a>9.3 反向传播算法的直观理解</h3><p>前项传播算法</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/63a0e4aef6d47ba7fa6e07088b61ae68.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    <span class="string">"""apply to architecture 400+1 * 25+1 *10</span></span><br><span class="line"><span class="string">    X: 5000 * 401</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    a1 = X  <span class="comment"># 5000 * 401</span></span><br><span class="line"></span><br><span class="line">    z2 = a1 @ t1.T  <span class="comment"># 5000 * 25</span></span><br><span class="line">    a2 = np.insert(sigmoid(z2), <span class="number">0</span>, np.ones(m), axis=<span class="number">1</span>)  <span class="comment"># 5000*26</span></span><br><span class="line"></span><br><span class="line">    z3 = a2 @ t2.T  <span class="comment"># 5000 * 10</span></span><br><span class="line">    h = sigmoid(z3)  <span class="comment"># 5000*10, this is h_theta(X)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a1, z2, a2, z3, h  <span class="comment"># you need all those for backprop</span></span><br></pre></td></tr></table></figure>

<p>反向传播算法</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/1542307ad9033e39093e7f28d0c7146c.png" alt="img"></p>
<p>类似于前项传播算法，只不过方向翻转了</p>
<p>以$\delta^{(2)}_2$为例，它的值为品红色和红色线上的权重与箭头所指$\delta$值对应相乘相加，如上图右上角所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># initialize</span></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    delta1 = np.zeros(t1.shape)  <span class="comment"># (25, 401)</span></span><br><span class="line">    delta2 = np.zeros(t2.shape)  <span class="comment"># (10, 26)</span></span><br><span class="line"></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        a1i = a1[i, :]  <span class="comment"># (1, 401)</span></span><br><span class="line">        z2i = z2[i, :]  <span class="comment"># (1, 25)</span></span><br><span class="line">        a2i = a2[i, :]  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        hi = h[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line">        yi = y[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line"></span><br><span class="line">        d3i = hi - yi  <span class="comment"># (1, 10)</span></span><br><span class="line"></span><br><span class="line">        z2i = np.insert(z2i, <span class="number">0</span>, np.ones(<span class="number">1</span>))  <span class="comment"># make it (1, 26) to compute d2i</span></span><br><span class="line">        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># careful with np vector transpose</span></span><br><span class="line">        delta2 += np.matrix(d3i).T @ np.matrix(a2i)  <span class="comment"># (1, 10).T @ (1, 26) -&gt; (10, 26)</span></span><br><span class="line">        delta1 += np.matrix(d2i[<span class="number">1</span>:]).T @ np.matrix(a1i)  <span class="comment"># (1, 25).T @ (1, 401) -&gt; (25, 401)</span></span><br><span class="line"></span><br><span class="line">    delta1 = delta1 / m</span><br><span class="line">    delta2 = delta2 / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br></pre></td></tr></table></figure>



<h3 id="9-4-实现注意：展开参数"><a href="#9-4-实现注意：展开参数" class="headerlink" title="9.4 实现注意：展开参数"></a>9.4 实现注意：展开参数</h3><p><strong>参数从矩阵展开成向量以便于传入高级优化函数</strong></p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/0ad78547859e6f794a7f18389d3d6128.png" alt="img"></p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/f9284204de41bffa4f7bc1dea567044e.png" alt="img"></p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/ebd7e196e272737f497853ba60743c44.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(seq)</span>:</span></span><br><span class="line"><span class="comment">#     """into ndarray of (25, 401), (10, 26)"""</span></span><br><span class="line">    <span class="keyword">return</span> seq[:<span class="number">25</span> * <span class="number">401</span>].reshape(<span class="number">25</span>, <span class="number">401</span>), seq[<span class="number">25</span> * <span class="number">401</span>:].reshape(<span class="number">10</span>, <span class="number">26</span>)</span><br></pre></td></tr></table></figure>



<h3 id="9-5-梯度检验"><a href="#9-5-梯度检验" class="headerlink" title="9.5 梯度检验"></a>9.5 梯度检验</h3><p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p>
<p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（<strong>Numerical Gradient Checking</strong>）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p>
<p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 $\theta$，我们计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/5d04c4791eb12a74c843eb5acf601400.png" alt="img"></p>
<p>当$\theta$是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对$\theta_1$进行检验的示例：$$ \frac{\partial}{\partial\theta_1}=\frac{J\left(\theta_1+\varepsilon_1,\theta_2,\theta_3…\theta_n \right)-J \left( \theta_1-\varepsilon_1,\theta_2,\theta_3…\theta_n \right)}{2\varepsilon} $$</p>
<p>最后我们还需要对通过反向传播方法计算出的偏导数进行检验。</p>
<p>根据上面的算法，计算出的偏导数存储在矩阵 $D_{ij}^{(l)}$ 中。检验时，我们要将该矩阵展开成为向量，同时我们也将 $\theta$ 矩阵展开为向量，我们针对每一个 $\theta$ 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同 $D_{ij}^{(l)}$ 进行比较。</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/bf65f3f3098025530a3c442eea562f8c.jpg" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_checking</span><span class="params">(theta, X, y, epsilon, regularized=False)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_numeric_grad</span><span class="params">(plus, minus, regularized=False)</span>:</span></span><br><span class="line">        <span class="string">"""calculate a partial gradient with respect to 1 theta"""</span></span><br><span class="line">        <span class="keyword">if</span> regularized:</span><br><span class="line">            <span class="keyword">return</span> (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    theta_matrix = expand_array(theta)  <span class="comment"># expand to (10285, 10285)</span></span><br><span class="line">    epsilon_matrix = np.identity(len(theta)) * epsilon</span><br><span class="line"></span><br><span class="line">    plus_matrix = theta_matrix + epsilon_matrix</span><br><span class="line">    minus_matrix = theta_matrix - epsilon_matrix</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate numerical gradient with respect to all theta</span></span><br><span class="line">    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># analytical grad will depend on if you want it to be regularized or not</span></span><br><span class="line">    analytic_grad = regularized_gradient(theta, X, y) <span class="keyword">if</span> regularized <span class="keyword">else</span> gradient(theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If you have a correct implementation, and assuming you used EPSILON = 0.0001</span></span><br><span class="line">    <span class="comment"># the diff below should be less than 1e-9</span></span><br><span class="line">    <span class="comment"># this is how original matlab code do gradient checking</span></span><br><span class="line">    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'</span>.format(diff))</span><br><span class="line"><span class="comment"># 运行比较慢</span></span><br></pre></td></tr></table></figure>



<h3 id="9-6-随机初始化"><a href="#9-6-随机初始化" class="headerlink" title="9.6 随机初始化"></a>9.6 随机初始化</h3><p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是<strong>对于神经网络来说是不可行的</strong>。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。</p>
<h3 id="9-7-综合起来"><a href="#9-7-综合起来" class="headerlink" title="9.7 综合起来"></a>9.7 综合起来</h3><p>小结一下使用神经网络时的步骤：</p>
<p>网络结构：</p>
<p>第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</p>
<ul>
<li><p>第一层的单元数即我们训练集的特征数量。</p>
</li>
<li><p>最后一层的单元数是我们训练集的结果的类的数量。</p>
</li>
<li><p>如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</p>
</li>
<li><p>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p>
</li>
</ul>
<p>训练神经网络：</p>
<ol>
<li>参数的随机初始化</li>
<li>利用正向传播方法计算所有的$h_{\theta}(x)​$</li>
<li>编写计算代价函数 $J​$ 的代码</li>
<li>利用反向传播方法计算所有偏导数</li>
<li>利用数值检验方法检验这些偏导数</li>
<li>使用优化算法来最小化代价函数</li>
</ol>
<h3 id="9-8-自主驾驶"><a href="#9-8-自主驾驶" class="headerlink" title="9.8 自主驾驶"></a>9.8 自主驾驶</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%A1%A8%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%A1%A8%E8%BF%B0/" itemprop="url">吴恩达机器学习（八）神经网络：表述</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:57:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第八、神经网络：表述"><a href="#第八、神经网络：表述" class="headerlink" title="第八、神经网络：表述"></a>第八、神经网络：表述</h2><h3 id="8-1-非线性假设"><a href="#8-1-非线性假设" class="headerlink" title="8.1 非线性假设"></a>8.1 非线性假设</h3><p>无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p>
<p>普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。</p>
<h3 id="8-2-神经元和大脑"><a href="#8-2-神经元和大脑" class="headerlink" title="8.2 神经元和大脑"></a>8.2 神经元和大脑</h3><p>神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。</p>
<p>也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。</p>
<p>从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。</p>
<h3 id="8-3-模型表示"><a href="#8-3-模型表示" class="headerlink" title="8.3 模型表示"></a>8.3 模型表示</h3><p>为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核（<strong>processing unit</strong>/<strong>Nucleus</strong>），它含有许多输入/树突（<strong>input</strong>/<strong>Dendrite</strong>），并且有一个输出/轴突（<strong>output</strong>/<strong>Axon</strong>）。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。</p>
<p><img src="%5Cimage%5C1581062470134.png" alt="1581062476782"></p>
<p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，<strong>activation unit</strong>）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（<strong>weight</strong>）。</p>
<p>神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（<strong>Input Layer</strong>），最后一层称为输出层（<strong>Output Layer</strong>），中间一层成为隐藏层（<strong>Hidden Layers</strong>）。</p>
<p><img src="%5Cimage%5C1581063001849.png" alt="1581063001849"></p>
<p>其中x1,x2,x3是输入单元（<strong>input units</strong>），我们将原始数据输入给它们。</p>
<p>a1,a2,a3是中间单元，它们负责将数据进行处理，然后呈递到下一层。 </p>
<p>最后是输出单元，它负责计算h(x)</p>
<p>如果为每一层都增加一个偏差单位（值永远为1）：</p>
<p><img src="%5Cimage%5C1581063118288.png" alt="1581063118288"></p>
<p><img src="%5Cimage%5C1581063330979.png" alt="1581063330979"></p>
<p>对于上图所示的模型，激活单元和输出分别表达为：</p>
<p><img src="/images/1584696145645.png" alt="1584696145645"></p>
<p><img src="%5Cimage%5C1581063445842.png" alt="1581063445842"></p>
<p>这样从左到右的算法称为<strong>前向传播算法</strong>( <strong>FORWARD PROPAGATION</strong> )</p>
<p>我们可以把a0,a1,a2,a3看成更为高级的特征值，也就是x0,x1,x2,x3的进化体，并且它们是由x决定的，因为是梯度下降的，所以a是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将x次方厉害，也能更好的预测新数据。 </p>
<p>这就是神经网络相比于逻辑回归和线性回归的优势。</p>
<p><img src="%5Cimage%5CIMG_0952(20200207-185347).PNG" alt=""></p>
<h3 id="8-4-样本和直观理解"><a href="#8-4-样本和直观理解" class="headerlink" title="8.4 样本和直观理解"></a>8.4 样本和直观理解</h3><p><img src="%5Cimage%5C1581067805852.png" alt="1581067805852"></p>
<p><img src="%5Cimage%5C1581067819352.png" alt="1581067819352"></p>
<p><img src="%5Cimage%5C1581067835185.png" alt="1581067835185"></p>
<p>我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实现<strong>XNOR</strong> 功能</p>
<p><img src="%5Cimage%5C1581067880235.png" alt="1581067880235"></p>
<p><img src="%5Cimage%5C1581067911722.png" alt="1581067911722"></p>
<p><img src="%5Cimage%5C1581067932063.png" alt="1581067932063"></p>
<h3 id="8-5-多类分类"><a href="#8-5-多类分类" class="headerlink" title="8.5 多类分类"></a>8.5 多类分类</h3><p>输出层将有多个神经元，我们得到的将会是一个n维矩阵</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E6%AD%A3%E5%88%99%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E6%AD%A3%E5%88%99%E5%8C%96/" itemprop="url">吴恩达机器学习（七）正则化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:55:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="七、正则化"><a href="#七、正则化" class="headerlink" title="七、正则化"></a>七、正则化</h2><h3 id="7-1-过拟合的问题（over-fitting）"><a href="#7-1-过拟合的问题（over-fitting）" class="headerlink" title="7.1 过拟合的问题（over-fitting）"></a>7.1 过拟合的问题（<strong>over-fitting</strong>）</h3><p>拟合有三种情况</p>
<ul>
<li><p>欠拟合</p>
</li>
<li><p>just right</p>
</li>
<li><p>过拟合（high variance）</p>
<p><img src="%5Cimage%5C580916143255.png" alt="1580916143255"></p>
</li>
</ul>
<p>这会导致训练出的模型无法用于其他的数据</p>
<p>问题是，如果我们发现了过拟合问题，应该如何处理？</p>
<ol>
<li><p>丢弃一些不能帮助我们正确预测的特征。</p>
<ul>
<li>手工选择保留哪些特征</li>
<li>使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</li>
</ul>
</li>
<li><p>正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</p>
</li>
</ol>
<h3 id="7-1-1-特征映射"><a href="#7-1-1-特征映射" class="headerlink" title="7.1.1 特征映射"></a>7.1.1 特征映射</h3><p><img src="%5Cimage%5C1580978577199.png" alt="1580978577199"></p>
<p>通过组合的方式将两个或多个系数转为多个各种阶数的系数</p>
<p>如上图是power = 6 的例子</p>
<h3 id="7-2-代价函数"><a href="#7-2-代价函数" class="headerlink" title="7.2 代价函数"></a>7.2 代价函数</h3><p>上面的回归问题中如果我们的模型是：</p>
<p><img src="%5Cimage%5Cwps2.jpg" alt="img"> </p>
<p>我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。</p>
<p>所以我们要做的就是在一定程度上<strong>减小这些高次参数的值</strong>，这就是正则化的基本方法。</p>
<p>也就是将cost function替换成下面这一个</p>
<p><img src="/images/1584696022284.png" alt="1584696022284"></p>
<p>以使得过拟合问题得到解决</p>
<p><img src="%5Cimage%5C1580917137232.png" alt="1580917137232"></p>
<p>对于正则化，我们要取一个合理的 $\lambda​$ 的值</p>
<h3 id="7-3-正则化线性回归"><a href="#7-3-正则化线性回归" class="headerlink" title="7.3 正则化线性回归"></a>7.3 正则化线性回归</h3><p>正则化线性回归的代价函数为：</p>
<p><img src="/images/1584696037200.png" alt="1584696037200"></p>
<p>如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对进行正则化，所以梯度下降算法将分两种情形：</p>
<p><img src="/images/1584696049338.png" alt="1584696049338"></p>
<p>对上面的算法中$ j=1,2,…,n$ 时的更新式子进行调整可得：</p>
<p><img src="/images/1584696066044.png" alt="1584696066044"></p>
<p>同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示：</p>
<p><img src="%5Cimage%5C1580976069023.png" alt="1580976069023"></p>
<p>图中的矩阵尺寸为 $(n+1)*(n+1)$</p>
<p>而且这样计算通过数学方法可以证明括号内的部分是可逆的</p>
<h3 id="7-4-正则化的逻辑回归模型"><a href="#7-4-正则化的逻辑回归模型" class="headerlink" title="7.4 正则化的逻辑回归模型"></a>7.4 正则化的逻辑回归模型</h3><p>自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：</p>
<p><img src="/images/1584696080243.png" alt="1584696080243"></p>
<p><strong>Python</strong>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costReg</span><span class="params">(theta, X, y, learningRate)</span>:</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))</span><br><span class="line">    second = np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - sigmoid(X*theta.T)))</span><br><span class="line">    reg = (learningRate / (<span class="number">2</span> * len(X))* np.sum(np.power(theta[:,<span class="number">1</span>:theta.shape[<span class="number">1</span>]],<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> np.sum(first - second) / (len(X)) + reg</span><br></pre></td></tr></table></figure>

<p>要最小化该代价函数，通过求导，得出梯度下降算法为：</p>
<p><img src="/images/1584696094154.png" alt="1584696094154"></p>
<p>注：看上去同线性回归一样，但是知道 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$，所以与线性回归不同。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" itemprop="url">吴恩达机器学习（六）逻辑回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:53:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="六、逻辑回归（Logistic-Regression）"><a href="#六、逻辑回归（Logistic-Regression）" class="headerlink" title="六、逻辑回归（Logistic Regression）"></a>六、逻辑回归（Logistic Regression）</h2><h3 id="6-1-分类问题"><a href="#6-1-分类问题" class="headerlink" title="6.1 分类问题"></a>6.1 分类问题</h3><p>算法的性质是：它的输出值永远在0到 1 之间。</p>
<p>当${h_\theta}\left( x \right)&gt;=0.5​$时，预测 $y=1​$。</p>
<p>当${h_\theta}\left( x \right)&lt;0.5$时，预测 $y=0$ 。</p>
<h3 id="6-2-逻辑函数"><a href="#6-2-逻辑函数" class="headerlink" title="6.2 逻辑函数"></a>6.2 逻辑函数</h3><p> <img src="%5Cimage%5CIMG_0947(20200204-193638).PNG" alt="IMG_0947(20200204-193638)"></p>
<p>$h_\theta \left( x \right)=g\left(\theta^{T}X \right)​$</p>
<p>$$g(z)=\frac{1}{1+{e}^{-z}}$$</p>
<p><strong>python</strong>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">   <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br></pre></td></tr></table></figure>

<h3 id="6-3-判定边界"><a href="#6-3-判定边界" class="headerlink" title="6.3 判定边界"></a>6.3 判定边界</h3><p>面对不同的训练集，我们需要用不同的参数来拟合，也就是我们所说的 decision boundary（判定边界）</p>
<p><img src="%5Cimage%5C2.png" alt="2"></p>
<p>而这样的需要用曲线拟合的</p>
<p><img src="%5Cimage%5C1580915252254.png" alt="1580915252254"></p>
<p><img src="%5Cimage%5C1580915344838.png" alt="1580915344838"></p>
<h3 id="6-4-代价函数（cost-function）"><a href="#6-4-代价函数（cost-function）" class="headerlink" title="6.4 代价函数（cost function）"></a>6.4 代价函数（cost function）</h3><p>由于sigmoid function的出现，会让以前的cost function出现很多个局部最小值</p>
<p><img src="%5Cimage%5Cobn.png" alt="obn"></p>
<p>我们希望的是得到一个差不多像是右边的那一条曲线而不是左边那一条</p>
<p> <img src="%5Cimage%5CIMG_0948(20200205-141243).PNG" alt="IMG_0948(20200205-141243)"></p>
<p>通过这样的操作，我们可以得到可以使用的cost function</p>
<p>$J( \theta)=\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{1}{2}{( {h_\theta}({x}^{(i)})-{y}^{(i)} )}^{2}}​$ </p>
<h3 id="6-5-简化的成本函数和梯度下降"><a href="#6-5-简化的成本函数和梯度下降" class="headerlink" title="6.5 简化的成本函数和梯度下降"></a>6.5 简化的成本函数和梯度下降</h3><ul>
<li><p>简化cost function（其实就是把原本两行的写成一行）</p>
<p><img src="/images/1584695848694.png" alt="1584695848694"></p>
<p><strong>Python</strong>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">  theta = np.matrix(theta)</span><br><span class="line">  X = np.matrix(X)</span><br><span class="line">  y = np.matrix(y)</span><br><span class="line">  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))</span><br><span class="line">  second = np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - sigmoid(X* theta.T)))</span><br><span class="line">  <span class="keyword">return</span> np.sum(first - second) / (len(X))</span><br></pre></td></tr></table></figure>

<p><img src="/images/1584695883696.png" alt="1584695883696"></p>
</li>
</ul>
<p>即使更新参数的规则看起来基本相同，但由于假设函数（h(x)）的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p>
<h3 id="6-6-高级优化"><a href="#6-6-高级优化" class="headerlink" title="6.6 高级优化"></a>6.6 高级优化</h3><p><strong>共轭梯度法 BFGS</strong> (<strong>变尺度法</strong>) 和<strong>L-BFGS</strong> (<strong>限制变尺度法</strong>) </p>
<p>例如可以使用 <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" target="_blank" rel="noopener"><code>scipy.optimize.minimize</code></a> 来寻找参数</p>
<h3 id="6-7-多类别分类：一对多"><a href="#6-7-多类别分类：一对多" class="headerlink" title="6.7 多类别分类：一对多"></a>6.7 多类别分类：一对多</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" itemprop="url">吴恩达机器学习（四）多变量线性回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:52:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>三讲的是线性代数基础，不做记录</p>
<h2 id="四、多变量线性回归"><a href="#四、多变量线性回归" class="headerlink" title="四、多变量线性回归"></a>四、多变量线性回归</h2><h3 id="4-1-多维特征与多变量梯度下降"><a href="#4-1-多维特征与多变量梯度下降" class="headerlink" title="4.1 多维特征与多变量梯度下降"></a>4.1 多维特征与多变量梯度下降</h3><p><img src="/image/IMG_0946(20200204-164453).PNG" alt="IMG_0946(images/IMG_0946(20200204-164453)-1584694213363.PNG)"></p>
<h3 id="4-2-梯度下降法实践1-特征缩放"><a href="#4-2-梯度下降法实践1-特征缩放" class="headerlink" title="4.2 梯度下降法实践1-特征缩放"></a>4.2 梯度下降法实践1-特征缩放</h3><p>Feature Scaling</p>
<p>保证这些特征都具有<strong>相近的尺度</strong>，这将帮助梯度下降算法更快地收敛。</p>
<p>我觉得其实就是正则化，将数据控制在 [-1，1] 之间，这样数据的差距就相对减小很多</p>
<p>当然这种方式是在不同的特征值的范围差距十分大的时候使用才有明显作用</p>
<p>eg: size = (0, 2000) ==&gt; size = (size - 1000) / 2000 ==&gt; size = (-1, 1)</p>
<p><strong>x = (x - 均值) / 标准差</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征缩放</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""Applies function along input axis(default 0) of DataFrame."""</span></span><br><span class="line">    <span class="keyword">return</span> df.apply(<span class="keyword">lambda</span> column: (column - column.mean()) / column.std())</span><br></pre></td></tr></table></figure>



<h3 id="4-3-梯度下降法实践2-学习率"><a href="#4-3-梯度下降法实践2-学习率" class="headerlink" title="4.3 梯度下降法实践2-学习率"></a>4.3 梯度下降法实践2-学习率</h3><p>如何选择学习率</p>
<p>看梯度下降的曲线图</p>
<p><img src="%5Cimage%5C%E5%AD%A6%E4%B9%A0%E7%8E%87.png" alt="学习率"></p>
<p>这张图中显然绿色线代表的学习率更好</p>
<h3 id="4-4-特征和多项式回归"><a href="#4-4-特征和多项式回归" class="headerlink" title="4.4 特征和多项式回归"></a>4.4 特征和多项式回归</h3><p> 其实就是可以通过将参数另外进行整理替换最后得到不同的回归方程的过程</p>
<p>如房价预测问题，可以将房子的长，宽两个参数整合成面积以方便拟合计算</p>
<p>$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}$ </p>
<p>${x_{1}}=frontage$（临街宽度），${x_{2}}=depth$（纵向深度），$x=frontage*depth=area$（面积），则：${h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x$。</p>
<h3 id="4-5-正规方程-Normal-equation"><a href="#4-5-正规方程-Normal-equation" class="headerlink" title="4.5 正规方程(Normal equation)"></a>4.5 正规方程(Normal equation)</h3><p>${\theta_{j}}:={\theta_{j}}-\alpha[\frac{1}{m}\sum\limits_{i = 1}^{m}h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j+\frac{\lambda}{m}\theta_j]$</p>
<table>
<thead>
<tr>
<th align="left">梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody><tr>
<td align="left">需要选择学习率α</td>
<td>不需要</td>
</tr>
<tr>
<td align="left">需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td align="left">当特征数量n大时也能较好适用</td>
<td>需要计算$(X^TX)^{-1}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当n小于10000 时还是可以接受的</td>
</tr>
<tr>
<td align="left">适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody></table>
<h3 id="4-6-正规方程及不可逆矩阵（选修）"><a href="#4-6-正规方程及不可逆矩阵（选修）" class="headerlink" title="4.6 正规方程及不可逆矩阵（选修）"></a>4.6 正规方程及不可逆矩阵（选修）</h3><h2 id="五、Octave教程-Octave-Tutorial"><a href="#五、Octave教程-Octave-Tutorial" class="headerlink" title="五、Octave教程(Octave Tutorial)"></a>五、Octave教程(Octave Tutorial)</h2><p>不做介绍</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" itemprop="url">吴恩达机器学习（二）单变量线性回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:46:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="二、单变量线性回归"><a href="#二、单变量线性回归" class="headerlink" title="二、单变量线性回归"></a>二、单变量线性回归</h2><h3 id="2-1-模型表示"><a href="#2-1-模型表示" class="headerlink" title="2.1 模型表示"></a>2.1 模型表示</h3><p>m代表训练集中实例的数量  </p>
<p>x代表特征/输入变量</p>
<p>y 代表目标变量/输出变量</p>
<p>(x,y)代表训练集中的实例</p>
<p>$ (x^{(i)}, y^{(i)}) ​$代表第i个观察实例</p>
<p>h 代表学习算法的解决方案或函数也称为假设（<strong>hypothesis</strong>）</p>
<h3 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2.2 代价函数"></a>2.2 代价函数</h3><p>假设函数：$ h_\theta(x) = \theta_0 + \theta_1x ​$</p>
<p>参数：$ \theta_0 , \theta_1 ​$</p>
<p>代价函数（cost function）: $ J(\theta_0 , \theta_1) = \frac{1}{2m} \sum\limits_{i = 1}^{m}h_\theta(x^{(i)}) - y^{(i)})^2​$</p>
<p>目标函数：$ minimize J(\theta_0 , \theta_1) $</p>
<h4 id="less-function"><a href="#less-function" class="headerlink" title="less function"></a>less function</h4><p>loss function通常用于衡量<strong>单个样本</strong>其预测值和实际值的“差距”</p>
<p>既然loss function是用于衡量预测值和实际值之间的”差距”，那么我们其实有很多的衡量手段，比如通过方差，如下： <strong>$ \varrho(\hat{y}^{(i)}, y^{(i)}) = \varrho(\hat{y}^{(i)} - y^{(i)})^2 $</strong></p>
<p>但是，在logistic regression算法中使用方差的方式无法得到凸函数(convex)，也就无法得到全局最小值，所以，我们在logistic regression中使用不同的loss function，如下： </p>
<p>$ \varrho(\hat{y}^{(i)}, y^{(i)}) =  -[y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)})log(1-log\hat{y}^{(i)})]$</p>
<p> 需要注意的是，不论是哪个函数，都是针对单个样本的，所以都带有上标 (i)</p>
<h4 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h4><p>cost function通常是针对<strong>样本集中的所有样本</strong>，而且是一个平均值。</p>
<p>cost function 是针对整个样本集的，因此它的计算公式需要将所有的loss function的结果进行加总然后求平均值，如下：</p>
<p>$ J(w, b) = \frac{1}{m}\sum\limits_{i = 1}^{m}[y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)})log(1-log\hat{y}^{(i)})] ​$ </p>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>得到最小的cost function值</p>
<h3 id="2-3-代价函数的直观理解I"><a href="#2-3-代价函数的直观理解I" class="headerlink" title="2.3 代价函数的直观理解I"></a>2.3 代价函数的直观理解I</h3><p>只有一个参数时</p>
<p>![cost function](\image\cost function.PNG)</p>
<p>即优化目标为1</p>
<h3 id="2-4-代价函数的直观理解II"><a href="#2-4-代价函数的直观理解II" class="headerlink" title="2.4 代价函数的直观理解II"></a>2.4 代价函数的直观理解II</h3><p>是一个两个参数的实例，实际上与 一个参数的类似，但是生成的cost function会多出一维（这很容易理解）（即变成三维曲面）</p>
<p><img src="%5Cimage%5C1580646267317.png" alt="1580646267317"></p>
<p>同样的，我们需要做的事情就是找出三维曲线中最小的值作为优化目标，当然，这个优化目标为二维的</p>
<h3 id="2-5-梯度下降"><a href="#2-5-梯度下降" class="headerlink" title="2.5 梯度下降"></a>2.5 梯度下降</h3><p>那么我们如何找出上面说的优化目标呢，用到的就是梯度下降算法</p>
<p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出<strong>代价函数的最小值</strong>。</p>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>
<p><img src="/images/1580647988012.png" alt="1580647988012"></p>
<p>想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。（不同的起点可能会导致不同的结果）</p>
<p>批量梯度下降（<strong>batch gradient descent</strong>）算法的定义：</p>
<p>${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta_0, \theta_1 \right)​$</p>
<p>实现：</p>
<p>​    $ temp0 := \theta_0 -  \alpha \frac{\partial }{\partial {\theta_{0}}}J\left(\theta_0, \theta_1 \right)​$</p>
<p>​    $ temp1 := \theta_1 -  \alpha \frac{\partial }{\partial {\theta_{1}}}J\left(\theta_0, \theta_1 \right) $</p>
<p>​    $ \theta_0 := temp0 ​$</p>
<p>​    $ \theta_1 := temp1 $</p>
<p>其中 $ \alpha$ 是学习率（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。</p>
<p>实现梯度下降算法的微妙之处是所有参数需要同时更新。</p>
<p>一般来说先将参数都初始化成0</p>
<h3 id="2-6-梯度下降的直观理解"><a href="#2-6-梯度下降的直观理解" class="headerlink" title="2.6 梯度下降的直观理解"></a>2.6 梯度下降的直观理解</h3><p><img src="%5Cimage%5CIMG_0945(20200202-212622).PNG" alt="IMG_0945(20200202-212622)"></p>
<p>如果 $ \alpha$ 太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果 α 太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。</p>
<p>如果 $ \alpha$ 太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果 α 太大，它会导致无法收敛，甚至发散。</p>
<p>如果将初始值放置在最低点，那么参数每次的变化将为0，但此时仍然进行着操作</p>
<p>在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，这时数值变化的也会变慢</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E5%BC%95%E8%A8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tiantian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天天的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E5%BC%95%E8%A8%80/" itemprop="url">吴恩达机器学习（一）引言</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:42:09+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、-引言"><a href="#一、-引言" class="headerlink" title="一、 引言"></a>一、 引言</h2><h3 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h3><h3 id="1-2-机器学习是什么？"><a href="#1-2-机器学习是什么？" class="headerlink" title="1.2 机器学习是什么？"></a>1.2 机器学习是什么？</h3><ol>
<li><p>介绍了机器学习初步的概念——一个不精通下棋的人写了个程序，程序自己和自己下了万把棋，记录下每把的经验最后该程序的棋技超过了编程者。</p>
</li>
<li><p>最近的定义——一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。</p>
</li>
</ol>
<h3 id="1-3-监督学习"><a href="#1-3-监督学习" class="headerlink" title="1.3 监督学习"></a>1.3 监督学习</h3><p>监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。</p>
<p>其实就是有标签的训练集，通过某种算法达成最好的拟合效果</p>
<ul>
<li>线性回归（连续的）</li>
<li>逻辑回归（离散的）</li>
</ul>
<h3 id="1-4-无监督学习"><a href="#1-4-无监督学习" class="headerlink" title="1.4 无监督学习"></a>1.4 无监督学习</h3><p>现实生活中常常会有这样的问题：</p>
<p>（1）缺乏足够的先验知识，因此难以人工标注类别;</p>
<p>（2）进行人工类别标注的成本太高。</p>
<p>很自然地，我们希望计算机能代我们(部分)完成这些工作，或至少提供一些帮助。常见的应用背景包括：</p>
<p>（1）一从庞大的样本集合中选出一些具有代表性的加以标注用于分类器的训练。</p>
<p>（2）先将所有样本自动分为不同的类别，再由人类对这些类别进行标注。</p>
<p>（3）在无类别信息情况下，寻找好的特征。</p>
<p>这样的操作，其实就是无监督学习</p>
<ul>
<li>通常使用聚类的方法来实现</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">tiantian</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7Carchive">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">tiantian</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
